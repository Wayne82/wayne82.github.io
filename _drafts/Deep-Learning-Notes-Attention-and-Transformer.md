---
layout: post
title:  "Deep Learning Notes: Attention and Transformer"
categories: neural-network
mathjax: true
comments: true
---

Eventually, my journey through neural networks has brought me to a point where I feel equipped with enough foundational knowledge to delve into the **attention mechanism** and **transformer models** in deep learning. I can’t help but start this post to record what I’ve learned so far, even as I continue exploring and deepening my understanding of this intriguing subject.

## Look Back Where I Started
Since this is a big milestone for me where I feel confident in understanding the 2 advanced topics, I want to take a moment to reflect on how I got here.

The first time I encountered the term "attention mechanism" and "transformer" was when chatGPT was released by OpenAI in November 2022. It caught everyone's attention, because it was a breakthrough in NLP and demonstrated impressive capabilities in generating human-like texts/responses. Particularly, it aroused my great interest and make me wonder how it worked behind the chat box. At that time, I have no idea about the terms of "Generative Pre-trained Transformer" from the name chatGPT itself, how it is related to the attention mechanism, and what is the famous "Attention is All You Need" paper about.

I know it is not an easy task to understand all the related techniques in depth, for someone like me who has no prior knowledge in NLP and deep learning, but has strong curiosity to dig into the mathematical level of details until I feel satisfied. It is about 2 years later, in the beginnig of this year 2025, I finally decided to take the challenge and started this learning journey.

My main approach is to start from the basics of neural networks, and follow the trace of deep learning advancements to learn the key techniques along the way and step by step ([Neural Network Basics and Backpropagation](https://wayne82.github.io/neural-network/2025/03/30/Neural-Network-Notes-The-Basics-and-Backpropagation.html), [Convolutional Neural Networks](https://wayne82.github.io/neural-network/2025/05/15/Neural-Network-Notes-Convolutional-Neural-Network.html), [Recurrent Neural Networks](https://wayne82.github.io/neural-network/2025/08/03/Neural-Network-Notes-Recurrent-Neural-Network-and-BPTT.html), and [NLP - Word2Vec Embedding](https://wayne82.github.io/neural-network/2025/10/12/Natural-Language-Processing-Notes-Word2Vec.html)), until I reach the attention mechanism and transformer models.

Now here I am.

## From RNN to Attention

## The Attention Mechanism

## Transformer Architecture

## Conclusion and Next Steps