---
layout: post
title:  "Natural Language Processing Notes: Word2Vec"
categories: neural-network
mathjax: true
comments: true
---

Now, I’ve set off on a new learning journey into Large Language Models (LLMs). As always, I prefer to begin with the fundamentals and gradually build up my understanding. Recently, I came across the Stanford course — [CS224N: NLP with Deep Learning](https://www.youtube.com/playlist?list=PLoROMvodv4rOaMFbaqxPDoLWjDaRAdP9D) — available on YouTube. After going through its syllabus, I found it to be an excellent resource for learning the core concepts of Natural Language Processing (NLP) and for preparing myself for more advanced topics related to LLMs.

The course begins with an introduction to the Word2Vec model — a foundational technique for learning word embeddings, which are numerical vector representations of words that capture their semantic meaning and relationships. It is quite a fundamental concept in NLP, yet I found it both interesting and a bit challenging to truly grasp. I spent some extra time going through the details, and eventually felt it would be worthwhile to write down my learning notes as a separate blog post.

## Linguistic Principle and Distributional Hypothesis

## What is Word2Vec Model and What Problem it Solves

## How the Word Embedding Vectors are Learnt

## Key Notes

## What is Next?